{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPxsJwOI18vib8+Y3sSn1Ch"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIQ5SkCTiwlg"
      },
      "source": [
        "#Setup\n",
        "Download NLTK data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13Ypa9q3Joa7"
      },
      "source": [
        "import nltk\n",
        "nltk.download('all')\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize, PunktSentenceTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import state_union\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "\n",
        "\n",
        "# # Upload from google drive\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n",
        "# print(\"len(uploaded.keys():\", len(uploaded.keys()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhTHKjM-BS9y"
      },
      "source": [
        "# !pip install -Uqq ipdb\n",
        "# import ipdb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSAtjc1XaOHq"
      },
      "source": [
        "# NLTK_tokens,stems"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrDuDGMcKJOI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f0737a4-1879-489c-ad70-a65dc2988e4e"
      },
      "source": [
        "# from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "# from nltk.corpus import stopwords\n",
        "# from nltk.stem import PorterStemmer\n",
        "\n",
        "#Pretty good regex required to get sentences out of it.\n",
        "example_text = \"Hello Mr. Sandy, how are you doing today? The weather is great and Python is awesome. The sky is pinkish-blue. You should not eat cardboard.\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "example_sent = \"This is a sample sentence, showing off the stop words filtration.\"\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "#print sent_tokenize(example_text)\n",
        "words =  word_tokenize(example_sent)\n",
        "\n",
        "filtered_sentence = [x for x in words if x not in stop_words]\n",
        "print (filtered_sentence)\n",
        "\n",
        "example_words = [\"python\",\"pythoner\",\"pythoning\",\"pythoned\",\"pythonly\"]\n",
        "\n",
        "ps = PorterStemmer()\n",
        "\n",
        "stemmed_example_words = [ps.stem(x) for x in example_words  ]\n",
        "\n",
        "print (stemmed_example_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['This', 'sample', 'sentence', ',', 'showing', 'stop', 'words', 'filtration', '.']\n",
            "['python', 'python', 'python', 'python', 'pythonli']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnMQLLZtaYV6"
      },
      "source": [
        "# NLTK_PartOfSpeechTagging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knafAIaWj5Sn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35c9265c-4efc-4f71-97be-d32562651158"
      },
      "source": [
        "# import nltk\n",
        "# from nltk.tokenize import sent_tokenize, word_tokenize, PunktSentenceTokenizer\n",
        "# from nltk.corpus import stopwords\n",
        "# from nltk.stem import PorterStemmer\n",
        "# from nltk.corpus import state_union\n",
        "\n",
        "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
        "sample_text = state_union.raw(\"2006-GWBush.txt\")\n",
        "\n",
        "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
        "\n",
        "tokeized = custom_sent_tokenizer.tokenize(sample_text)\n",
        "\n",
        "def process_content():\n",
        "    try:\n",
        "        for i in tokeized:\n",
        "            words = nltk.word_tokenize(i)\n",
        "            tagged = nltk.pos_tag(words)\n",
        "            #print tagged\n",
        "            #Chunking\n",
        "            #chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP><NN>?}\"\"\"\n",
        "            #Chinking\n",
        "            chunkGram = r\"\"\"Chunk: {<.*>+}  \n",
        "                            }<VB.?|IN|DT|TO>+{\"\"\"\n",
        "            chunkParser = nltk.RegexpParser(chunkGram)\n",
        "            chunked = chunkParser.parse(tagged)\n",
        "            \n",
        "            print (chunked)\n",
        "            \n",
        "            \n",
        "            \n",
        "    except Exception as e:\n",
        "        print (str(e))      \n",
        "\n",
        "# process_content()\n",
        "\n",
        "\n",
        "words = nltk.word_tokenize(tokeized[0])\n",
        "tagged = nltk.pos_tag(words)\n",
        "print(tagged)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('PRESIDENT', 'NNP'), ('GEORGE', 'NNP'), ('W.', 'NNP'), ('BUSH', 'NNP'), (\"'S\", 'POS'), ('ADDRESS', 'NNP'), ('BEFORE', 'IN'), ('A', 'NNP'), ('JOINT', 'NNP'), ('SESSION', 'NNP'), ('OF', 'IN'), ('THE', 'NNP'), ('CONGRESS', 'NNP'), ('ON', 'NNP'), ('THE', 'NNP'), ('STATE', 'NNP'), ('OF', 'IN'), ('THE', 'NNP'), ('UNION', 'NNP'), ('January', 'NNP'), ('31', 'CD'), (',', ','), ('2006', 'CD'), ('THE', 'NNP'), ('PRESIDENT', 'NNP'), (':', ':'), ('Thank', 'NNP'), ('you', 'PRP'), ('all', 'DT'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uWMfe5caioh"
      },
      "source": [
        "# NLTK_NamedEntityRecognition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QiGmPzMdkP21"
      },
      "source": [
        "def process_content_NE():\n",
        "    try:\n",
        "        for i in tokeized:\n",
        "            words = nltk.word_tokenize(i)\n",
        "            tagged = nltk.pos_tag(words)\n",
        "\n",
        "            namedEnt = nltk.ne_chunk(tagged)\n",
        "            \n",
        "            print (namedEnt)\n",
        "            \n",
        "    except Exception as e:\n",
        "        print (str(e) )       \n",
        "\n",
        "process_content_NE()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sFvfLXDakYO"
      },
      "source": [
        "# NLTK_wordnet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsliA-j3kqUA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d25d9bfa-8fc6-4342-ee79-74152d8389ee"
      },
      "source": [
        "#from nltk.corpus import wordnet\n",
        "\n",
        "syns = wordnet.synsets(\"program\")\n",
        "print (syns)\n",
        "#synset\n",
        "print(syns[0].name())\n",
        "print(syns[0].lemmas())\n",
        "#just the word\n",
        "print(syns[0].lemmas()[0].name())\n",
        "#definition\n",
        "print(syns[2].definition() )\n",
        "#examples\n",
        "print(syns[2].examples()) #list - can be one or more examples"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Synset('plan.n.01'), Synset('program.n.02'), Synset('broadcast.n.02'), Synset('platform.n.02'), Synset('program.n.05'), Synset('course_of_study.n.01'), Synset('program.n.07'), Synset('program.n.08'), Synset('program.v.01'), Synset('program.v.02')]\n",
            "plan.n.01\n",
            "[Lemma('plan.n.01.plan'), Lemma('plan.n.01.program'), Lemma('plan.n.01.programme')]\n",
            "plan\n",
            "a radio or television show\n",
            "['did you see his program last night?']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMTjaFM0g-0F"
      },
      "source": [
        "synonyms = []\n",
        "antonyms = []\n",
        "# print(wordnet.synsets(\"good\")[1].lemmas()[0].antonyms())\n",
        "for syn in wordnet.synsets(\"good\"):\n",
        "  # print(\"Syn = \",syn)\n",
        "  for l in syn.lemmas():\n",
        "    synonyms.append(l.name())\n",
        "    # print(\"Lemma =\", l)\n",
        "    # print(\"Antonyms =\",l.antonyms())\n",
        "    if l.antonyms():\n",
        "      antonyms.append(l.antonyms()[0].name())\n",
        "\n",
        "# print(set(synonyms))\n",
        "# print(set(antonyms))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8AXecxxRaM6",
        "outputId": "deb755d2-2a93-40ae-b87a-952dc276b656"
      },
      "source": [
        "w1 = wordnet.synset(\"ship.n.01\")\n",
        "w2 = wordnet.synset(\"water.n.01\")\n",
        "print(w1.wup_similarity(w2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.23529411764705882\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5rIWirkXQgj"
      },
      "source": [
        "# NLTK_TextClassification using Custom Voting Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqOi_Sv9XVTu"
      },
      "source": [
        "import random\n",
        "from nltk.corpus import movie_reviews\n",
        "import pickle\n",
        "#Import drive\n",
        "from google.colab import drive\n",
        "#Mount Google Drive\n",
        "drive.mount(\"/content/drive\")\n",
        "base_path = './drive/MyDrive/Colab Notebooks/Data/SentimentAnalysisSentdex/'\n",
        "\n",
        "selection = int(input(\"Which data to train upon? 1.Corpus 2.Sentdex\"))\n",
        "\n",
        "def get_corpus_data():\n",
        "  #this is a list of tuples - (review words, category)\n",
        "  documents = [(list(movie_reviews.words(fileid)),category)\n",
        "                for category in movie_reviews.categories()\n",
        "                for fileid in movie_reviews.fileids(category)]\n",
        "\n",
        "  # print(documents[0])\n",
        "  #get all words from neg and pos folder\n",
        "  all_words = []\n",
        "  for w in movie_reviews.words():\n",
        "    all_words.append(w.lower())\n",
        "  return all_words , documents\n",
        "\n",
        "def get_own_data():\n",
        "\n",
        "  short_pos = open(base_path + 'positive.txt', 'r',encoding='latin-1').read()\n",
        "  short_neg = open(base_path + 'negative.txt', 'r',encoding='latin-1').read()\n",
        "\n",
        "  documents = []\n",
        "  all_words = []\n",
        "\n",
        "  #  j is adject, r is adverb, and v is verb\n",
        "  #allowed_word_types = [\"J\",\"R\",\"V\"]\n",
        "  allowed_word_types = [\"J\"]\n",
        "\n",
        "  for r in short_pos.split('\\n'):\n",
        "    documents.append( (r, 'pos'))\n",
        "    words = word_tokenize(r)\n",
        "    pos = nltk.pos_tag(words)\n",
        "    for w in pos:\n",
        "        if w[1][0] in allowed_word_types:\n",
        "            all_words.append(w[0].lower())\n",
        "  \n",
        "  for r in short_neg.split('\\n'):\n",
        "    documents.append( (r, 'neg')) \n",
        "    words = word_tokenize(r)\n",
        "    pos = nltk.pos_tag(words)\n",
        "    for w in pos:\n",
        "        if w[1][0] in allowed_word_types:\n",
        "            all_words.append(w[0].lower())  \n",
        "\n",
        "  return all_words, documents\n",
        "\n",
        "all_words , documents = get_corpus_data() if selection == 1 else get_own_data()\n",
        "all_words = nltk.FreqDist(all_words)\n",
        "#print(all_words.most_common(15) )  \n",
        "# ipdb.set_trace()\n",
        "if selection == 1:\n",
        "  word_features = list(all_words.keys())[:3000]\n",
        "else:\n",
        "  word_features = list(all_words.keys())[:5000]\n",
        "\n",
        "#pickle everything to save time\n",
        "save_documents = open(base_path + \"pickled_algos/documents.pickle\",\"wb\")\n",
        "pickle.dump(documents, save_documents)\n",
        "save_documents.close()\n",
        "save_word_features = open(base_path + \"pickled_algos/word_features5k.pickle\",\"wb\")\n",
        "pickle.dump(word_features, save_word_features)\n",
        "save_word_features.close()\n",
        "\n",
        "#Create featureset with list of most common words and their boolean\n",
        "# whether it's present in review or not.\n",
        "def find_features(document):\n",
        "  if selection == 1:\n",
        "    words = set(document)\n",
        "  else: #document is a line\n",
        "    words = word_tokenize(document)  \n",
        "  features = {}\n",
        "  for w in word_features:\n",
        "    features[w] = (w in words)\n",
        "  return features  \n",
        "\n",
        "featuresets = [(find_features(doc), category) for (doc,category) in documents]\n",
        "# random.shuffle(featuresets)\n",
        "save_featuresets = open(base_path + \"pickled_algos/featuresets.pickle\",\"wb\")\n",
        "pickle.dump(featuresets, save_featuresets)\n",
        "save_featuresets.close()\n",
        "\n",
        "if selection == 1:\n",
        "  training_set = featuresets[:1900] \n",
        "  testing_set = featuresets[1900:]\n",
        "else:\n",
        "  training_set = featuresets[:10000] \n",
        "  testing_set = featuresets[10000:]\n",
        "\n",
        "clf = nltk.NaiveBayesClassifier.train(training_set)\n",
        "\n",
        "print(\"Accuracy :\", nltk.classify.accuracy(clf, testing_set))\n",
        "print(clf.show_most_informative_features(15))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W04P5nsLrLlS"
      },
      "source": [
        "Own Voted Classifier using ScikitLearn wrapper "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6t75f_GpsQnF"
      },
      "source": [
        "\"\"\"\n",
        "SklearnClassifier is a wrapper to bridge nltk and sklearn so that we\n",
        "can directly use nltk functions.\n",
        "\"\"\"\n",
        "from nltk.classify.scikitlearn import SklearnClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB\n",
        "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
        "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
        "from nltk.classify import ClassifierI\n",
        "from collections import Counter\n",
        "\n",
        "class VoteClassifier(ClassifierI): #Overridden ClassifierI?\n",
        "  def __init__(self, *classifiers):\n",
        "    self._classifiers = classifiers\n",
        "\n",
        "  def classify(self, features):\n",
        "    votes = []\n",
        "    for c in self._classifiers:\n",
        "      v = c.classify(features)\n",
        "      votes.append(v)\n",
        "    return Counter(votes).most_common()[0][0]\n",
        "\n",
        "  def confidence(self, features):\n",
        "    votes = []\n",
        "    for c in self._classifiers:\n",
        "      v = c.classify(features)\n",
        "      votes.append(v)\n",
        "    \n",
        "    choice_votes = votes.count(Counter(votes).most_common()[0][0])\n",
        "    conf = choice_votes / len(votes)\n",
        "    return conf \n",
        "\n",
        "clf = nltk.NaiveBayesClassifier.train(training_set)\n",
        "print(\"Original NLTK Clf Accuracy :\", nltk.classify.accuracy(clf, testing_set))\n",
        "\n",
        "save_classifier = open(base_path + \"pickled_algos/originalnaivebayes5k.pickle\",\"wb\")\n",
        "pickle.dump(clf, save_classifier)\n",
        "save_classifier.close()\n",
        "\n",
        "MNB_clf = SklearnClassifier(MultinomialNB())\n",
        "MNB_clf.train(training_set)\n",
        "print(\"MNB Accuracy :\", nltk.classify.accuracy(MNB_clf, testing_set))\n",
        "\n",
        "save_classifier = open(base_path + \"pickled_algos/MNB_classifier5k.pickle\",\"wb\")\n",
        "pickle.dump(MNB_clf, save_classifier)\n",
        "save_classifier.close()\n",
        "\n",
        "# GNB_clf = SklearnClassifier(GaussianNB())\n",
        "# GNB_clf.train(training_set)\n",
        "# print(\"GNB Accuracy :\", nltk.classify.accuracy(GNB_clf, testing_set))\n",
        "\n",
        "BNB_clf = SklearnClassifier(BernoulliNB())\n",
        "BNB_clf.train(training_set)\n",
        "print(\"BNB Accuracy :\", nltk.classify.accuracy(BNB_clf, testing_set))\n",
        "\n",
        "save_classifier = open(base_path + \"pickled_algos/BernoulliNB_classifier5k.pickle\",\"wb\")\n",
        "pickle.dump(BNB_clf, save_classifier)\n",
        "save_classifier.close()\n",
        "\n",
        "LogReg_clf = SklearnClassifier(LogisticRegression())\n",
        "LogReg_clf.train(training_set)\n",
        "print(\"LogReg_clf Accuracy :\", nltk.classify.accuracy(LogReg_clf, testing_set))\n",
        "\n",
        "save_classifier = open(base_path + \"pickled_algos/LogisticRegression_classifier5k.pickle\",\"wb\")\n",
        "pickle.dump(LogReg_clf, save_classifier)\n",
        "save_classifier.close()\n",
        "\n",
        "SGDClassifier_clf = SklearnClassifier(SGDClassifier())\n",
        "SGDClassifier_clf.train(training_set)\n",
        "print(\"SGDClassifier Accuracy :\", nltk.classify.accuracy(SGDClassifier_clf, testing_set))\n",
        "\n",
        "save_classifier = open(base_path + \"pickled_algos/SGDC_classifier5k.pickle\",\"wb\")\n",
        "pickle.dump(SGDClassifier_clf, save_classifier)\n",
        "save_classifier.close()\n",
        "\n",
        "# SVC_clf = SklearnClassifier(SVC())\n",
        "# SVC_clf.train(training_set)\n",
        "# print(\"SVC Accuracy :\", nltk.classify.accuracy(SVC_clf, testing_set))\n",
        "\n",
        "LinearSVC_clf = SklearnClassifier(LinearSVC())\n",
        "LinearSVC_clf.train(training_set)\n",
        "print(\"LinearSVC_clf Accuracy :\", nltk.classify.accuracy(LinearSVC_clf, testing_set))\n",
        "\n",
        "save_classifier = open(base_path + \"pickled_algos/LinearSVC_classifier5k.pickle\",\"wb\")\n",
        "pickle.dump(LinearSVC_clf, save_classifier)\n",
        "save_classifier.close()\n",
        "\n",
        "# NuSVC_clf = SklearnClassifier(NuSVC())\n",
        "# NuSVC_clf.train(training_set)\n",
        "# print(\"NuSVC_clf Accuracy :\", nltk.classify.accuracy(NuSVC_clf, testing_set))\n",
        "\n",
        "\n",
        "voted_classifier = VoteClassifier(clf,MNB_clf,BNB_clf, LogReg_clf,\n",
        "                                  LinearSVC_clf)\n",
        "\n",
        "#This worked bcz may be it is implemented same as nltk classifiers with functions classify and all.\n",
        "print(\"voted_classifier Accuracy :\", nltk.classify.accuracy(voted_classifier, testing_set))\n",
        "\n",
        "print(\"Classification:\", voted_classifier.classify(testing_set[0][0]), \n",
        "      \"Confidence :\",  voted_classifier.confidence(testing_set[0][0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c6Y3mvPJFbP"
      },
      "source": [
        "# Pickled Voted Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jS7ZNR1CJKOd",
        "outputId": "6cbe9b7e-3666-48c8-d55d-64c0abd1ac5d"
      },
      "source": [
        "import nltk\n",
        "import random\n",
        "#from nltk.corpus import movie_reviews\n",
        "from nltk.classify.scikitlearn import SklearnClassifier\n",
        "import pickle\n",
        "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
        "from nltk.classify import ClassifierI\n",
        "from statistics import mode\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "#Import drive\n",
        "from google.colab import drive\n",
        "#Mount Google Drive\n",
        "drive.mount(\"/content/drive\")\n",
        "base_path = './drive/MyDrive/Colab Notebooks/Data/SentimentAnalysisSentdex/'\n",
        "\n",
        "class VoteClassifier(ClassifierI): #Overridden ClassifierI?\n",
        "  def __init__(self, *classifiers):\n",
        "    self._classifiers = classifiers\n",
        "\n",
        "  def classify(self, features):\n",
        "    votes = []\n",
        "    for c in self._classifiers:\n",
        "      v = c.classify(features)\n",
        "      votes.append(v)\n",
        "    return Counter(votes).most_common()[0][0]\n",
        "\n",
        "  def confidence(self, features):\n",
        "    votes = []\n",
        "    for c in self._classifiers:\n",
        "      v = c.classify(features)\n",
        "      votes.append(v)\n",
        "    \n",
        "    choice_votes = votes.count(Counter(votes).most_common()[0][0])\n",
        "    conf = choice_votes / len(votes)\n",
        "    return conf \n",
        "\n",
        "\n",
        "documents_f = open(base_path + \"pickled_algos/documents.pickle\", \"rb\")\n",
        "documents = pickle.load(documents_f)\n",
        "documents_f.close()\n",
        "\n",
        "\n",
        "word_features5k_f = open(base_path + \"pickled_algos/word_features5k.pickle\", \"rb\")\n",
        "word_features = pickle.load(word_features5k_f)\n",
        "word_features5k_f.close()\n",
        "\n",
        "\n",
        "def find_features(document):\n",
        "    words = word_tokenize(document)\n",
        "    features = {}\n",
        "    for w in word_features:\n",
        "        features[w] = (w in words)\n",
        "\n",
        "    return features\n",
        "\n",
        "featuresets_f = open(base_path + \"pickled_algos/featuresets.pickle\", \"rb\")\n",
        "featuresets = pickle.load(featuresets_f)\n",
        "featuresets_f.close()\n",
        "\n",
        "random.shuffle(featuresets)\n",
        "print(len(featuresets))\n",
        "\n",
        "testing_set = featuresets[10000:]\n",
        "training_set = featuresets[:10000]\n",
        "\n",
        "\n",
        "\n",
        "open_file = open(base_path + \"pickled_algos/originalnaivebayes5k.pickle\", \"rb\")\n",
        "classifier = pickle.load(open_file)\n",
        "open_file.close()\n",
        "\n",
        "\n",
        "open_file = open(base_path + \"pickled_algos/MNB_classifier5k.pickle\", \"rb\")\n",
        "MNB_classifier = pickle.load(open_file)\n",
        "open_file.close()\n",
        "\n",
        "\n",
        "\n",
        "open_file = open(base_path + \"pickled_algos/BernoulliNB_classifier5k.pickle\", \"rb\")\n",
        "BernoulliNB_classifier = pickle.load(open_file)\n",
        "open_file.close()\n",
        "\n",
        "\n",
        "open_file = open(base_path + \"pickled_algos/LogisticRegression_classifier5k.pickle\", \"rb\")\n",
        "LogisticRegression_classifier = pickle.load(open_file)\n",
        "open_file.close()\n",
        "\n",
        "\n",
        "open_file = open(base_path + \"pickled_algos/LinearSVC_classifier5k.pickle\", \"rb\")\n",
        "LinearSVC_classifier = pickle.load(open_file)\n",
        "open_file.close()\n",
        "\n",
        "\n",
        "open_file = open(base_path + \"pickled_algos/SGDC_classifier5k.pickle\", \"rb\")\n",
        "SGDC_classifier = pickle.load(open_file)\n",
        "open_file.close()\n",
        "\n",
        "\n",
        "voted_classifier = VoteClassifier(\n",
        "                                  classifier,\n",
        "                                  LinearSVC_classifier,\n",
        "                                  MNB_classifier,\n",
        "                                  BernoulliNB_classifier,\n",
        "                                  LogisticRegression_classifier)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def sentiment(text):\n",
        "    feats = find_features(text)\n",
        "    return voted_classifier.classify(feats),voted_classifier.confidence(feats)\n",
        "\n",
        "print(sentiment(\"This movie was awesome! The acting was great, plot was wonderful, and there were pythons...so yea!\"))\n",
        "print(sentiment(\"This movie was utter junk. There were absolutely 0 pythons. I don't see what the point was at all. Horrible movie, 0/10\"))    \n",
        "print(sentiment(\"wow its nice\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "10664\n",
            "('pos', 1.0)\n",
            "('neg', 1.0)\n",
            "('pos', 1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FOSj-BBTk-k"
      },
      "source": [
        "# Stanford NER Tagger"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40aB_GLDTskC",
        "outputId": "6a3fa3e6-2157-43ee-c8c1-caa97010e219"
      },
      "source": [
        "# An alternative to NLTK's named entity recognition (NER) classifier is provided by the Stanford NER tagger. This tagger is largely seen as the standard in named entity recognition, but since it uses an advanced statistical learning algorithm it's more computationally expensive than the option provided by NLTK.\n",
        "\n",
        "#     3 class model for recognizing locations, persons, and organizations\n",
        "#     4 class model for recognizing locations, persons, organizations, and miscellaneous entities\n",
        "#     7 class model for recognizing locations, persons, organizations, times, money, percents, and dates\n",
        "\n",
        "from nltk.tag import StanfordNERTagger\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "st = StanfordNERTagger('./drive/MyDrive/Colab Notebooks/Data/stanford-ner/classifiers/english.all.3class.distsim.crf.ser.gz',\n",
        "\t\t\t\t\t   './drive/MyDrive/Colab Notebooks/Data/stanford-ner/stanford-ner.jar',\n",
        "\t\t\t\t\t   encoding='utf-8')\n",
        "\n",
        "text = 'While in France, Christine Lagarde discussed short-term stimulus efforts in a recent interview with the Wall Street Journal.'\n",
        "\n",
        "tokenized_text = word_tokenize(text)\n",
        "classified_text = st.tag(tokenized_text)\n",
        "\n",
        "print(classified_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/tag/stanford.py:183: DeprecationWarning: \n",
            "The StanfordTokenizer will be deprecated in version 3.2.5.\n",
            "Please use \u001b[91mnltk.tag.corenlp.CoreNLPPOSTagger\u001b[0m or \u001b[91mnltk.tag.corenlp.CoreNLPNERTagger\u001b[0m instead.\n",
            "  super(StanfordNERTagger, self).__init__(*args, **kwargs)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[('While', 'O'), ('in', 'O'), ('France', 'LOCATION'), (',', 'O'), ('Christine', 'PERSON'), ('Lagarde', 'PERSON'), ('discussed', 'O'), ('short-term', 'O'), ('stimulus', 'O'), ('efforts', 'O'), ('in', 'O'), ('a', 'O'), ('recent', 'O'), ('interview', 'O'), ('with', 'O'), ('the', 'O'), ('Wall', 'ORGANIZATION'), ('Street', 'ORGANIZATION'), ('Journal', 'ORGANIZATION'), ('.', 'O')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doMi-JPD0TTT"
      },
      "source": [
        "# Replacing words with regex\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "F3Byt_CB0kjs",
        "outputId": "af275aeb-4690-488f-f4b6-8de71f97556b"
      },
      "source": [
        "import re\n",
        "replacement_patterns = [ (r'won\\'t', 'will not'), \n",
        "                        (r'can\\'t', 'cannot'), \n",
        "                        (r'i\\'m', 'i am'), \n",
        "                        (r'ain\\'t', 'is not'), \n",
        "                        (r'(\\w+)\\'ll', '\\g<1> will'), #group 1? \n",
        "                        (r'(\\w+)n\\'t', '\\g<1> not'), \n",
        "                        (r'(\\w+)\\'ve', '\\g<1> have'), \n",
        "                        (r'(\\w+)\\'s', '\\g<1> is'), \n",
        "                        (r'(\\w+)\\'re', '\\g<1> are'), \n",
        "                        (r'(\\w+)\\'d', '\\g<1> would')\n",
        "                        ]\n",
        "\n",
        "class RegexpReplacer(object):\n",
        "\n",
        "  def __init__(self, patterns=replacement_patterns):\n",
        "    self.patterns = [(re.compile(regex), repl) for (regex, repl) in patterns]\n",
        "  \n",
        "  def replace(self, text): \n",
        "    s = text\n",
        "    for (pattern, repl) in self.patterns: \n",
        "      s = re.sub(pattern, repl, s)\n",
        "    return s  \n",
        "\n",
        "replacer = RegexpReplacer()\n",
        "replacer.replace(\"can't and should've are contractions\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'cannot and should have are contractions'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxJiJ3wT9ex4"
      },
      "source": [
        "# Removing repeating characters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "pTIS6fo59i5J",
        "outputId": "d94dd7b9-42f3-43ce-ace8-f4c0efe0fa24"
      },
      "source": [
        "import re\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "class RepeatReplacer(object):\n",
        "  def __init__(self):\n",
        "    self.repeat_regexp = re.compile(r'(\\w*)(\\w)\\2(\\w*)') \n",
        "    self.repl = r'\\1\\2\\3'\n",
        "\n",
        "  def replace(self, word):\n",
        "    if wordnet.synsets(word):\n",
        "      return word\n",
        "    repl_word = self.repeat_regexp.sub(self.repl, word)\n",
        "    if repl_word != word:\n",
        "      return self.replace(repl_word)\n",
        "    else:\n",
        "      return repl_word      \n",
        "\n",
        "replacer = RepeatReplacer() \n",
        "replacer.replace('whaaaaaaatttt')         "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'what'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    }
  ]
}